{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c19edf-f419-4d70-94ac-3201f01c1521",
   "metadata": {},
   "source": [
    "# Using LLama Factory finetune on SageMaker \n",
    "# 3. 使用SageMaker LMI(Large Model Inference) vLLM 引擎部署模型至SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c8dfa-c07f-4db6-b463-3cf8432828f7",
   "metadata": {},
   "source": [
    "The LMI container offers the out-of-box integration with SageMaker for hosting multiple LoRA adapters with higher performance (low latency and high throughput) using the vLLM library that uses S-LORA and Punica. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead.\n",
    "\n",
    "Below diagram shows the Multi LoRA-Adapter serving stack of LMI container on SageMaker Multi LoRA-Adapter serving stack of LMI container on SageMaker  \n",
    "![imge](https://raw.githubusercontent.com/aws-samples/sagemaker-genai-hosting-examples/0a98859eef9c53a5aa3beeae7d59b38a8de934dc/Llama2/Llama2-7b/LMI/LoRA-LMI-SageMaker.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cdc3f0a-5e2f-4fcb-a17c-872a10c2f620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "default_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d07ddd-1ba1-4736-8f96-cee8ba4c5af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aa0fd1-0c7b-44fd-94e4-36d4ff86bfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124-v1.0\n"
     ]
    }
   ],
   "source": [
    "# inference_image_uri = image_uris.retrieve(\n",
    "#     framework=\"djl-deepspeed\",\n",
    "#     region=sess.boto_session.region_name,\n",
    "#     version=\"0.27.0\"\n",
    "# )\n",
    "inference_image_uri = (\n",
    "    \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124-v1.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec4b57a-fe88-4874-b649-07cfd1bcc07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_code_dir = 'vllm_inference'\n",
    "!mkdir -p {local_code_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd5706-737f-4d21-81f7-aaf56d9257e5",
   "metadata": {},
   "source": [
    "* Note: option.model_id 需要改成模型下载的s3_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2ecdf9-14bc-4f01-a5f2-d5c091aaddd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vllm_inference/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {local_code_dir}/serving.properties\n",
    "engine=Python\n",
    "option.model_id=TechxGenus/Meta-Llama-3-8B-Instruct-AWQ\n",
    "option.dtype=fp16\n",
    "option.enable_lora=true\n",
    "option.rolling_batch=vllm\n",
    "option.tensor_parallel_degree=1\n",
    "option.max_model_len=8192\n",
    "option.max_tokens=8192\n",
    "option.output_formatter = json\n",
    "option.model_loading_timeout = 1200\n",
    "option.max_rolling_batch_size=64\n",
    "option.max_cpu_loras=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef411ce7-7da5-4ce0-847c-874f321e0490",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 创建lora目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6b4a0d-a80b-45c9-98ca-9289da66a00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {local_code_dir}/adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36bfbc-6014-4a8d-b03c-24c7753e4507",
   "metadata": {},
   "source": [
    "### 下载训练好的Lora至本地目录打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23d8f59-78d7-4eae-89b8-1857e28e8e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-577976195821/llama3-8b-qlora/finetuned_model/runs/Jun26_14-15-21_algo-1-nescd/events.out.tfevents.1719412318.algo-1-nescd.216.1 to vllm_inference/adapters/exp/runs/Jun26_14-15-21_algo-1-nescd/events.out.tfevents.1719412318.algo-1-nescd.216.1\n",
      "download: s3://sagemaker-us-east-1-577976195821/llama3-8b-qlora/finetuned_model/runs/Jun26_14-15-21_algo-1-nescd/events.out.tfevents.1719411354.algo-1-nescd.216.0 to vllm_inference/adapters/exp/runs/Jun26_14-15-21_algo-1-nescd/events.out.tfevents.1719411354.algo-1-nescd.216.0\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://{default_bucket}/llama3-8b-qlora/finetuned_model/ {local_code_dir}/adapters/exp\n",
    "!rm -rf {local_code_dir}/adapters/exp/checkpoint*\n",
    "!rm -rf {local_code_dir}/adapters/exp/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e573a8ed-ea46-4406-8f3c-e889f4dd34f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm_inference/\n",
      "vllm_inference/serving.properties\n",
      "vllm_inference/adapters/\n",
      "vllm_inference/adapters/exp/\n",
      "vllm_inference/adapters/exp/all_results.json\n",
      "vllm_inference/adapters/exp/README.md\n",
      "vllm_inference/adapters/exp/eval_results.json\n",
      "vllm_inference/adapters/exp/training_args.bin\n",
      "vllm_inference/adapters/exp/trainer_state.json\n",
      "vllm_inference/adapters/exp/train_results.json\n",
      "vllm_inference/adapters/exp/special_tokens_map.json\n",
      "vllm_inference/adapters/exp/adapter_model.safetensors\n",
      "vllm_inference/adapters/exp/trainer_log.jsonl\n",
      "vllm_inference/adapters/exp/adapter_config.json\n",
      "vllm_inference/adapters/exp/training_loss.png\n",
      "vllm_inference/adapters/exp/tokenizer.json\n",
      "vllm_inference/adapters/exp/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!cd {local_code_dir} && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf model.tar.gz {local_code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6bf418f-e299-40fb-8372-701e7a7cad06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_code_prefix: llm_finetune/llama-3-8b-qlora\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"llm_finetune/llama-3-8b-qlora\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f44d531-4dca-4b24-8a60-a4656f8b29e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-577976195821/llm_finetune/llama-3-8b-qlora/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", default_bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfcd19-29fc-438e-8e03-81bae79ea020",
   "metadata": {},
   "source": [
    "### 创建SageMaker模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c66e321-d9ae-4a42-907a-bde3e19f6d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-8b-qlora-vllm-2024-06-26-14-39-03-287\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124-v1.0\n",
      "Created Model: arn:aws:sagemaker:us-east-1:577976195821:model/llama3-8b-qlora-vllm-2024-06-26-14-39-03-287\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = name_from_base(f\"llama3-8b-qlora-vllm\").replace('.','-').replace('_','-')\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78c3a4-85b1-477f-ad77-a23e5c6f033a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 创建SageMaker端点模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e9ec38-96f8-47fc-8ac3-23257fa64f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:577976195821:endpoint-config/llama3-8b-qlora-vllm-2024-06-26-14-39-03-287-config',\n",
       " 'ResponseMetadata': {'RequestId': 'df1f01e9-2b23-4d1f-8d50-4f7148e7bfa1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'df1f01e9-2b23-4d1f-8d50-4f7148e7bfa1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '132',\n",
       "   'date': 'Wed, 26 Jun 2024 14:39:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "#Note: ml.g4dn.2xlarge 也可以选择\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 10*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355233c6-79e2-4119-9d22-b44d2562ef2d",
   "metadata": {},
   "source": [
    "### 创建SageMaker端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c9ba70c-6465-40ac-a065-b0dd98326484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-east-1:577976195821:endpoint/llama3-8b-qlora-vllm-2024-06-26-14-39-03-287-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cc7a0-d925-4238-88b1-178f190618e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 大概等到8分钟左右节点部署成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83307f44-8971-4f48-92be-098a6eb07f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:577976195821:endpoint/llama3-8b-qlora-vllm-2024-06-26-14-39-03-287-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eac2e8-5971-45af-8c05-645bf010e816",
   "metadata": {},
   "source": [
    "## 调用SageMaker Endpoint 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f06f456f-a37b-4b2e-abc3-23d4a3199f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 ms, sys: 2.53 ms, total: 4.04 ms\n",
      "Wall time: 2.94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "  \"max_new_tokens\": 512,\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\":0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434728f2-b5f2-4b3d-8216-13a92bc756ab",
   "metadata": {},
   "source": [
    "### 加载Tokenizer， 使用其chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9dfa1c7-5765-4175-ba57-d94cebea254b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af891f4233d04522987350a7cc48b581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77e7f63ea8a4df7b2a14bb7bcc4e176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7b39010aed45ba8815a7c53d7bae8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba6a97-2d0a-4ddf-85a4-2919b557ce51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7faae325-242f-4ba1-be61-1568c1e5f746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "NEWLINE = re.compile(r'\\\\n')  \n",
    "DOUBLE_NEWLINE = re.compile(r'\\\\n\\\\n')\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream from Llama 2 model inferenced with LMI Container. \n",
    "    \n",
    "    The output of the model will be in the following repetetive but incremental format:\n",
    "    ```\n",
    "    b'{\"generated_text\": \"'\n",
    "    b'lo from L\"'\n",
    "    b'LM \\\\n\\\\n'\n",
    "    b'How are you?\"}'\n",
    "    ...\n",
    "\n",
    "    For each iteration, we just read the incremental part and seek for the new position for the next iteration till the end of the line.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        start_sequence = b'{\"generated_text\": \"'\n",
    "        stop_sequence = b'\"}'\n",
    "        new_line = '\\n'\n",
    "        double_new_line = '\\n\\n'\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line:\n",
    "                self.read_pos += len(line)\n",
    "                if line.startswith(start_sequence):# in :\n",
    "                    line = line.lstrip(start_sequence)\n",
    "\n",
    "                if line.endswith(stop_sequence):\n",
    "                    line =line.rstrip(stop_sequence)\n",
    "                line = line.decode('utf-8')\n",
    "                line = NEWLINE.sub(new_line, line)\n",
    "                line = DOUBLE_NEWLINE.sub(double_new_line, line)\n",
    "                return line\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09f6715d-3bee-4557-a835-0469c3c212d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#测试第一个消息\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":\"请始终用中文回答\"},\n",
    "     {\"role\": \"user\", \"content\": \"你是谁？你是干嘛的\"},\n",
    "]\n",
    "\n",
    "# 测试第二个消息\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\":\"请始终用中文回答\"},\n",
    "#      {\"role\": \"user\", \"content\": \"睡觉时被女鬼压床我该怎么办？\"},\n",
    "# ]\n",
    "\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67394413-7d67-4e8c-8a39-acbc72aa9920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        \"max_new_tokens\":512, \n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.95,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57f7e1-3d17-43e4-bfe1-4f71d6d50b9a",
   "metadata": {},
   "source": [
    "## 不使用Lora的情况下测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3a64cc-2cfe-41cb-afea-325c8f23a20f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是 LLaMA，一个由 Meta 开发的基于人工智能的语言模型。我可以理解和生成自然语言，帮助用户回答问题、完成任务、甚至进行对话。\n",
      "\n",
      "我可以用来做很多事情，例如：\n",
      "\n",
      "* 解答问题：我可以回答各种问题，包括历史、科学、技术、娱乐等领域。\n",
      "* 翻译：我可以将文本翻译成多种语言。\n",
      "* 文本生成：我可以生成文本，包括文章、故事、诗歌等。\n",
      "* 对话：我可以与用户进行对话，回答问题、提供信息和建议。\n",
      "\n",
      "我是一个机器人，我的目的是帮助用户获取信息、完成任务和提高语言能力。"
     ]
    }
   ],
   "source": [
    "response_stream = smr_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": inputs,\n",
    "                \"parameters\":parameters,\n",
    "                \"stream\" : True,\n",
    "                # \"adapters\":\"exp\" # Lora Adapter\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "            CustomAttributes='accept_eula=false'\n",
    "        )\n",
    "\n",
    "for token in LineIterator(response_stream[\"Body\"]):\n",
    "    # pass\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3201f5a-3b19-418a-a385-48c65e30430a",
   "metadata": {},
   "source": [
    "## 使用Lora的情况下测试\n",
    "- 通过指定 \"adapters\":\"exp\" 加载lora，如果有多个lora模型，也可以实现不同lora模型之间的切换\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08e3c31a-4bff-4777-b1d1-af5ce9f68c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好，我是 RiverBot，一个由 GOGOGO 开发的人工智能助手，我可以帮您回答各种问题，提供实用的建议和帮助。"
     ]
    }
   ],
   "source": [
    "response_stream = smr_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": inputs,\n",
    "                \"parameters\":parameters,\n",
    "                \"stream\" : True,\n",
    "                \"adapters\":\"exp\" # Lora Adapter\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "            CustomAttributes='accept_eula=false'\n",
    "        )\n",
    "\n",
    "for token in LineIterator(response_stream[\"Body\"]):\n",
    "    # pass\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30b3b3-2249-4ee7-9d4c-1565edc51e8a",
   "metadata": {},
   "source": [
    "## ！！！！实验结束之后，运行下面命令删除节点！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "635e958b-6768-4e70-a9bb-03762dc577bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name {endpoint_name}\n",
    "!aws sagemaker delete-endpoint-config --endpoint-config-name {endpoint_config_name}\n",
    "!aws sagemaker delete-model --model-name {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6b4e5-1674-4c99-a38c-ca22825ffc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
