{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLama Factory finetune on SageMaker - Multi Nodes\n",
    "# 4. 使用SageMaker Training Job分布式训练\n",
    "- 本示例使用deepspeed进行多机多GPU卡分布式训练\n",
    "- 如果指定instance数量为1，则是单机多GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Pre-requisites 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.101 requires botocore==1.34.101, but you have botocore 1.34.122 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker boto3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "### 数据集1. 从huggingface上下载ruozhiba数据集\n",
    "- 该数据集有近5k条数据，本次实验我们可以只用前1k条做训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a74513d0f404d98a6a4dbf276cb565e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9a4d24f79f4dce865110ccfefd84ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caccf901b9345daac8cab5ba4e7ab45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6e4bcec0db4100959f2764ac7a3684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4898\n",
      "\n",
      "Training sample:\n",
      "\n",
      "{'output': '人类消化系统并不能处理硬物如银行卡。吃银行卡这种行为不仅危险，还可能导致健康问题，甚至需要医疗干预。银行卡内的钱是一种电子数据，和卡的物理形态没有直接关系，因此无论你如何处理银行卡，都无法直接“获得”卡内的钱。你应该去银行或相关金融机构，用正规的方式提取或转移账户内的资金。切勿尝试吞食等危险举动。', 'input': '', 'instruction': '吃了好几张卡也没吐钱 是我吃的姿势不对吗？'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "dataset_name = \"hfl/ruozhiba_gpt4\"\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'output': '这个问题是针对《海绵宝宝》中的蟹老板和他的快餐店“蟹堡王”的情节。在动画中，蟹黄堡是蟹堡王里最受欢迎的食品，但是具体配方是保密的。虽然蟹老板是一只蟹，但是该动画并未明确指出蟹黄堡是使用蟹肉制作的。事实上，这更多是一种戏剧化的设定，用来增加情节的幽默感。因此，从字面上来理解蟹老板制作蟹黄堡吃掉同类是不正确的，这只是一种讽刺的艺术表现手法，旨在娱乐观众而已。', 'input': '', 'instruction': '为什么蟹老板做蟹黄堡蚕食它的同类？'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集2. 身份数据集\n",
    "```json\n",
    "[{'instruction': 'hi',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'hello',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'Who are you?',\n",
    "  'input': '',\n",
    "  'output': 'I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'}]\n",
    "```\n",
    "把其中的name和author替换成您自己想替换的值，这样微调完成之后，问模型“你是谁，谁创造的你？”这类的身份问题，模型就会按这个新的值来回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_identity(origin_obj,name,author):\n",
    "    ret = []\n",
    "    for ele in origin_obj:\n",
    "        ele['output'] = ele['output'].replace(\"{{name}}\",name).replace(\"{{author}}\",author)\n",
    "        ret.append(ele)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 替换成您自己的设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME = 'RiverBot'\n",
    "AUTHOR = 'GOGOGO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'hi',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am RiverBot, an AI assistant developed by River. How can I assist you today?'},\n",
       " {'instruction': 'hello',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am RiverBot, an AI assistant developed by River. How can I assist you today?'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name = './LLaMA-Factory/data/identity.json'\n",
    "with open(file_name) as f:\n",
    "    identity = json.load(f)\n",
    "\n",
    "identity_2 = format_identity(identity,name='RiverBot',author='River')\n",
    "identity_2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('./train',exist_ok=True)\n",
    "with open('./train/identity_2.json','w') as f:\n",
    "    json.dump(identity_2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据copy至S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_data_uri = f\"s3://{default_bucket}/dataset-for-training\"\n",
    "training_input_path = f'{s3_data_uri}/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45efe9f15dd042f8932ec7ae5bfdf254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving training dataset to: s3://sagemaker-us-west-2-434444145045/dataset-for-training/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "train_dataset.to_json('./train/ruozhiba.json')\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/ruozhiba.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/identity_2.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = './LLaMA-Factory/data/dataset_info.json'\n",
    "with open(file_name) as f:\n",
    "    datainfo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['identity']={'file_name': 'identity_2.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['ruozhiba']={\n",
    "    'file_name':'ruozhiba.json',\n",
    "    \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\",\n",
    "  }      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./LLaMA-Factory/data/dataset_info.json','w') as f:\n",
    "    json.dump(fp=f,obj=datainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 训练配置yaml文件\n",
    "### 从LLaMA-Factory/examples/lora_multi_gpu/目录中复制出llama3_lora_sft_ds.yaml，并修改\n",
    "- 本次实验是使用Lora训练\n",
    "- 如果用全量微调，则使用LLaMA-Factory/examples/full_multi_gpu/llama3_full_sft.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'q_proj,v_proj',\n",
       " 'ddp_timeout': 180000000,\n",
       " 'deepspeed': 'examples/deepspeed/ds_z3_config.json',\n",
       " 'dataset': 'identity,alpaca_en_demo',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 1024,\n",
       " 'max_samples': 1000,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': 'saves/llama3-8b/lora/sft',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 2,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'fp16': True,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'evaluation_strategy': 'steps',\n",
       " 'eval_steps': 500}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load template\n",
    "import yaml\n",
    "file_name = './LLaMA-Factory/examples/lora_multi_gpu/llama3_lora_sft_ds.yaml'\n",
    "with open(file_name) as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本次实验我们使用原始精度的LLaMA-3-8b， 从hf的repo 'unsloth/llama-3-8b-Instruct' 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'unsloth/llama-3-8b-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'unsloth/llama-3-8b-Instruct',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'q_proj,v_proj',\n",
       " 'ddp_timeout': 180000000,\n",
       " 'deepspeed': 'examples/deepspeed/ds_z3_config.json',\n",
       " 'dataset': 'identity,ruozhiba',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 2048,\n",
       " 'max_samples': 1000,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': '/tmp/finetuned_model',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 2,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 3,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'fp16': True,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'evaluation_strategy': 'steps',\n",
       " 'eval_steps': 500,\n",
       " 'warmup_steps': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc['model_name_or_path'] = model_id\n",
    "doc['output_dir'] ='/tmp/finetuned_model'\n",
    "doc['num_train_epochs'] = 3\n",
    "doc['warmup_steps'] = 10\n",
    "doc['per_device_train_batch_size'] =1\n",
    "doc['gradient_accumulation_steps'] =2\n",
    "# doc['lora_target'] = 'all'\n",
    "doc['cutoff_len'] = 2048\n",
    "#实验时间，只选取前1000条数据做训练\n",
    "doc['max_samples'] = 1000\n",
    "doc['dataset'] = 'identity,ruozhiba'\n",
    "doc['eval_steps'] = 500\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sg_config = 'sg_config_multl_node_lora_ds.yaml'\n",
    "with open(f'./LLaMA-Factory/{sg_config}', 'w') as f:\n",
    "    yaml.safe_dump(doc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置Lora权重Merge 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = './LLaMA-Factory/examples/merge_lora/llama3_lora_sft.yaml'\n",
    "with open(file_name) as f:\n",
    "    doc2 = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sg_lora_merge_config = 'sg_config_lora_merge.yaml'\n",
    "doc2['model_name_or_path'] = model_id\n",
    "doc2['adapter_name_or_path'] ='/tmp/finetuned_model'\n",
    "doc2['export_dir'] ='/tmp/finetuned_model_merged'\n",
    "with open(f'./LLaMA-Factory/{sg_lora_merge_config}', 'w') as f:\n",
    "    yaml.safe_dump(doc2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交训练任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型输出s3目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_s3 = f's3://{default_bucket}/llama3-8b-lora-sft-ds/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240608152854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-8b-instruct-finetune-2024-06-08-15-28-54-208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:28:58 Starting - Starting the training job\n",
      "2024-06-08 15:28:58 Pending - Training job waiting for capacity.........\n",
      "2024-06-08 15:30:27 Pending - Preparing the instances for training.........\n",
      "2024-06-08 15:31:39 Downloading - Downloading input data...\n",
      "2024-06-08 15:32:04 Downloading - Downloading the training image............\n",
      "2024-06-08 15:34:04 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-08 15:35:04,239 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-08 15:35:04,301 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 15:35:04,313 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-08 15:35:04,315 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-08 15:35:06,208 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-06-08 15:35:08,441 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-06-08 15:35:08,504 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 15:35:08,515 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-06-08 15:35:08,517 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-06-08 15:35:10,256 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers>=4.41.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 4.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.16.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.30.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft>=0.11.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl>=0.8.6 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio>=4.0.0 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading gradio-4.36.0-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.7.2)\u001b[0m\n",
      "\u001b[34mCollecting fastapi (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting sse-starlette (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.0-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting fire (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading fire-0.6.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.2.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 63.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting transformers>=4.41.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 2.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets>=2.16.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting accelerate>=0.30.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting peft>=0.11.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mCollecting trl>=0.8.6 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[35mCollecting gradio>=4.0.0 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading gradio-4.36.0-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting tiktoken (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\u001b[0m\n",
      "\u001b[35mCollecting uvicorn (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.7.2)\u001b[0m\n",
      "\u001b[35mCollecting fastapi (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[35mCollecting sse-starlette (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[35mDownloading sse_starlette-2.1.0-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.4)\u001b[0m\n",
      "\u001b[35mCollecting fire (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[35mDownloading fire-0.6.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 15.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (23.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\u001b[0m\n",
      "\u001b[35mCollecting deepspeed (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.14.2.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting autoawq (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting metrics (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.3.3.tar.gz (18 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting rouge-chinese (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 62.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.23.0 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mCollecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting altair<6.0,>=4.2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.2.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting autoawq (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[35mDownloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[35mCollecting metrics (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.3.3.tar.gz (18 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==1.0.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpx>=0.24.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.7/49.7 kB 6.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting rouge-chinese (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[35mDownloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting jieba (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[35mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 73.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mCollecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.11.0)\u001b[0m\n",
      "\u001b[34mCollecting urllib3~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.18.3)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.38.0,>=0.37.2 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastapi-cli>=0.0.2 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting email_validator>=2.0.0 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.23.0 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mCollecting anyio (from sse-starlette->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.52.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting termcolor (from fire->-r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting autoawq-kernels (from autoawq->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mDownloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting Pygments==2.2.0 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading Pygments-2.2.0-py2.py3-none-any.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathspec==0.5.5 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading pathspec-0.5.5.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pathlib2>=2.3.0 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.22.0)\u001b[0m\n",
      "\u001b[34mCollecting toolz (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.7)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.4.1 (from transformers>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\u001b[0m\n",
      "\u001b[35mCollecting pyarrow-hotfix (from datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[35mCollecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mCollecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting sniffio (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[35mCollecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting altair<6.0,>=4.2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading ffmpy-0.3.2.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting gradio-client==1.0.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.5.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.5.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is still looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mDownloading rich-13.4.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py<3.0.0,>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading rich-13.4.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.3.5-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.3.4-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.3.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[34mDownloading rich-13.3.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.3.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.3.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.2.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.1.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting commonmark<0.10.0,>=0.9.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading rich-13.0.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.0.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.5.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.5.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.4.4-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.4.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.4.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.4.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.4.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.3.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.2.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.0.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-12.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-11.2.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.6)\u001b[0m\n",
      "\u001b[34mDownloading rich-11.1.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading rich-11.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.16.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.16.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.16.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting httpx>=0.24.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\u001b[0m\n",
      "\u001b[35mCollecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.7/49.7 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\u001b[0m\n",
      "\u001b[35mCollecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[35mCollecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.11.0)\u001b[0m\n",
      "\u001b[35mCollecting urllib3~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\u001b[0m\n",
      "\u001b[35mCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.18.3)\u001b[0m\n",
      "\u001b[35mCollecting starlette<0.38.0,>=0.37.2 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting fastapi-cli>=0.0.2 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting email_validator>=2.0.0 (from fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\u001b[0m\n",
      "\u001b[35mCollecting anyio (from sse-starlette->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[35mDownloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.52.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting termcolor (from fire->-r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[35mDownloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[35mCollecting hjson (from deepspeed->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (1.11.1.1)\u001b[0m\n",
      "\u001b[35mCollecting py-cpuinfo (from deepspeed->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from deepspeed->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.22.0)\u001b[0m\n",
      "\u001b[35mCollecting autoawq-kernels (from autoawq->-r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[35mDownloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.15.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.15.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.15.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.14.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.13.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.12.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-10.11.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting metrics (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.3.2.tar.gz (18 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting Pygments==2.2.0 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading Pygments-2.2.0-py2.py3-none-any.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting pathspec==0.5.5 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading pathspec-0.5.5.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting pathlib2>=2.3.0 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.22.0)\u001b[0m\n",
      "\u001b[35mCollecting toolz (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[35mCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.7)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.3.1.tar.gz (14 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\u001b[0m\n",
      "\u001b[35mCollecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[35mCollecting sniffio (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[35mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.3.0.tar.gz (14 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[35mCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.35.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[35mINFO: pip is looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[35mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.5.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.5.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mINFO: pip is still looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[35mDownloading rich-13.4.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.2.8.tar.gz (12 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pathspec==0.5.3 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading pathspec-0.5.3.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting markdown-it-py<3.0.0,>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading rich-13.4.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.3.5-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.3.4-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.3.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[35mDownloading rich-13.3.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.3.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.3.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.2.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.1.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting commonmark<0.10.0,>=0.9.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading rich-13.0.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-13.0.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.5.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.5.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.4.4-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.4.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.4.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.4.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.4.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.3.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.2.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.0.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-12.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-11.2.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.6)\u001b[0m\n",
      "\u001b[35mDownloading rich-11.1.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading rich-11.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.16.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.16.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.16.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.15.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.15.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.15.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.14.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.13.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.12.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading rich-10.11.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting metrics (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.3.2.tar.gz (18 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mCollecting metrics (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.2.7.tar.gz (12 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.3.1.tar.gz (14 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mDownloading metrics-0.2.6.tar.gz (12 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.18.0)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.3.0.tar.gz (14 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 87.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.1/542.1 kB 41.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.4/309.4 kB 32.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 26.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.4-py3-none-any.whl (226 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio-4.36.0-py3-none-any.whl (12.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 80.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.0.1-py3-none-any.whl (318 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.1/318.1 kB 35.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 61.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 56.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 8.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.111.0-py3-none-any.whl (91 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\u001b[0m\n",
      "\u001b[34mDownloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 9.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.2.8.tar.gz (12 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting pathspec==0.5.3 (from metrics->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading pathspec-0.5.3.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 15.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading altair-5.3.0-py3-none-any.whl (857 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 857.8/857.8 kB 49.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading email_validator-2.1.1-py3-none-any.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 65.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 7.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 9.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 12.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 32.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.5/142.5 kB 18.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 48.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 77.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 61.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 11.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.4.0-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 12.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 76.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading typer-0.12.3-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.2/47.2 kB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.6/53.6 kB 7.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 17.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl (33.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.4/33.4 MB 47.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 27.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 31.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.4/341.4 kB 32.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 82.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35mCollecting metrics (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.2.7.tar.gz (12 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mDownloading metrics-0.2.6.tar.gz (12 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.18.0)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 65.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 16.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 35.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.1/56.1 kB 9.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: llamafactory, fire, deepspeed, metrics, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for llamafactory: filename=llamafactory-0.7.2.dev0-py3-none-any.whl size=177759 sha256=5b5dbe0d7d7cd8c6ec42d1b8959fc5263561f982152ae205a73a95e24b4a3e79\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-k9tytqhu/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=1fc440ca92a45fc7917093d390c83b29a84bcdf6a2e08f70df754fbb08823097\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 102.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.1/542.1 kB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.4/309.4 kB 41.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 28.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading trl-0.9.4-py3-none-any.whl (226 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading gradio-4.36.0-py3-none-any.whl (12.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 89.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-1.0.1-py3-none-any.whl (318 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.1/318.1 kB 43.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 75.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 67.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 11.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.111.0-py3-none-any.whl (91 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 17.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\u001b[0m\n",
      "\u001b[35mDownloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 20.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mDownloading altair-5.3.0-py3-none-any.whl (857 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 857.8/857.8 kB 66.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading email_validator-2.1.1-py3-none-any.whl (30 kB)\u001b[0m\n",
      "\u001b[35mDownloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[35mDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 73.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 7.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 8.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 45.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.5/142.5 kB 25.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 55.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 71.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 69.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 14.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading anyio-4.4.0-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 10.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 85.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading typer-0.12.3-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.2/47.2 kB 5.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.6/53.6 kB 8.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl (33.4 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.4/33.4 MB 53.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 8.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 10.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 30.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 39.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 36.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.4/341.4 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 97.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 74.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 39.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.1/56.1 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: llamafactory, fire, deepspeed, metrics, jieba, ffmpy\u001b[0m\n",
      "\u001b[35mBuilding wheel for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for llamafactory: filename=llamafactory-0.7.2.dev0-py3-none-any.whl size=177759 sha256=59114f140f9d3b5db6e6a7570dd7eba3af5dcfa6197f0fa709eb6a5b691d90ee\u001b[0m\n",
      "\u001b[35mStored in directory: /tmp/pip-ephem-wheel-cache-f5rwn5ek/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=581ba8a2d4e5b42193e6cd78aa902665922975dc893d5ff82d712fedb90c814c\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432246 sha256=46bd228182d6807b26041b17ca73fb657502613e613e9a34b774bb85d23df03f\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ea/7c/43/bed44d8414c099ff962b754f425f7ff77cc623cc8a98e0da70\u001b[0m\n",
      "\u001b[34mBuilding wheel for metrics (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for metrics (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for metrics: filename=metrics-0.2.6-py3-none-any.whl size=12791 sha256=82128c89001fa76d769c3649db4c35e68f4e26ff59838678a94bc811342399b0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/c5/ca/d1/efae53ccccfba939be676f12c0449626e9da6ec1f6ec6e6e3d\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314459 sha256=2513bc40bb79e7dae27e19e73c40d31a0bb05b5b23c35f7339c7810c25908eb0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=cb539080b85283e9ff76fea131916346eca81fe589d7e1561ef62ce6f45886b2\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\u001b[0m\n",
      "\u001b[34mSuccessfully built llamafactory fire deepspeed metrics jieba ffmpy\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432240 sha256=4682e6468ff1ed2ce874d82ed248081c7f509908a0e9d261430ac63f21890cd7\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/ea/7c/43/bed44d8414c099ff962b754f425f7ff77cc623cc8a98e0da70\u001b[0m\n",
      "\u001b[35mBuilding wheel for metrics (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for metrics (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for metrics: filename=metrics-0.2.6-py3-none-any.whl size=12791 sha256=2c438767b9c9017b01f2a6bdb239848313b9867ad503a8f9fc3c1da339ae3800\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/c5/ca/d1/efae53ccccfba939be676f12c0449626e9da6ec1f6ec6e6e3d\u001b[0m\n",
      "\u001b[35mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, pydub, py-cpuinfo, jieba, hjson, ffmpy, xxhash, websockets, uvloop, urllib3, ujson, toolz, tomlkit, termcolor, sniffio, shtab, semantic-version, safetensors, ruff, rouge-chinese, regex, python-multipart, python-dotenv, pynvml, pyarrow-hotfix, orjson, multidict, metrics, importlib-resources, httptools, h11, fsspec, frozenlist, docstring-parser, dnspython, async-timeout, aiofiles, yarl, uvicorn, httpcore, fire, email_validator, anyio, aiosignal, watchfiles, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, deepspeed, bitsandbytes, autoawq-kernels, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi-cli, altair, accelerate, transformers, fastapi, datasets, trl, peft, gradio, autoawq, llamafactory\u001b[0m\n",
      "\u001b[35mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314459 sha256=bf7be41a2c839c58b15c835a2d1c8865bd0880bb38f2901ec13ea1db40c7c4f2\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.18\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.18:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.18\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2024.5.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2024.5.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2024.5.0\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=f0bf7208dbb02ddd4c122b19dd412ea1317ae45282754ae5f34c708a25e80f64\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\u001b[0m\n",
      "\u001b[35mSuccessfully built llamafactory fire deepspeed metrics jieba ffmpy\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typer\u001b[0m\n",
      "\u001b[34mFound existing installation: typer 0.9.4\u001b[0m\n",
      "\u001b[34mUninstalling typer-0.9.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typer-0.9.4\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sentencepiece, pydub, py-cpuinfo, jieba, hjson, ffmpy, xxhash, websockets, uvloop, urllib3, ujson, toolz, tomlkit, termcolor, sniffio, shtab, semantic-version, safetensors, ruff, rouge-chinese, regex, python-multipart, python-dotenv, pynvml, pyarrow-hotfix, orjson, multidict, metrics, importlib-resources, httptools, h11, fsspec, frozenlist, docstring-parser, dnspython, async-timeout, aiofiles, yarl, uvicorn, httpcore, fire, email_validator, anyio, aiosignal, watchfiles, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, deepspeed, bitsandbytes, autoawq-kernels, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi-cli, altair, accelerate, transformers, fastapi, datasets, trl, peft, gradio, autoawq, llamafactory\u001b[0m\n",
      "\u001b[35mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[35mFound existing installation: urllib3 1.26.18\u001b[0m\n",
      "\u001b[35mUninstalling urllib3-1.26.18:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled urllib3-1.26.18\u001b[0m\n",
      "\u001b[35mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[35mFound existing installation: fsspec 2024.5.0\u001b[0m\n",
      "\u001b[35mUninstalling fsspec-2024.5.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled fsspec-2024.5.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: typer\u001b[0m\n",
      "\u001b[35mFound existing installation: typer 0.9.4\u001b[0m\n",
      "\u001b[35mUninstalling typer-0.9.4:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled typer-0.9.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.31.0 aiofiles-23.2.1 aiohttp-3.9.5 aiosignal-1.3.1 altair-5.3.0 anyio-4.4.0 async-timeout-4.0.3 autoawq-0.2.5 autoawq-kernels-0.0.6 bitsandbytes-0.43.1 datasets-2.19.2 deepspeed-0.14.2 dnspython-2.6.1 docstring-parser-0.16 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 frozenlist-1.4.1 fsspec-2024.3.1 gradio-4.36.0 gradio-client-1.0.1 h11-0.14.0 hjson-3.1.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.3 importlib-resources-6.4.0 jieba-0.42.1 llamafactory-0.7.2.dev0 metrics-0.2.6 multidict-6.0.5 orjson-3.10.3 peft-0.11.1 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pydub-0.25.1 pynvml-11.5.0 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 rouge-chinese-1.0.3 ruff-0.4.8 safetensors-0.4.3 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.0 starlette-0.37.2 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 transformers-4.41.2 trl-0.9.4 typer-0.12.3 tyro-0.8.4 ujson-5.10.0 urllib3-2.2.1 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:02,887 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:02,887 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:02,978 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:03,060 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:03,138 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:03,152 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"llama3-8b-instruct-finetune-2024-06-08-15-28-54-208\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-434444145045/llama3-8b-instruct-finetune-2024-06-08-15-28-54-208/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry-multi-nodes\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry-multi-nodes.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry-multi-nodes.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry-multi-nodes\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-434444145045/llama3-8b-instruct-finetune-2024-06-08-15-28-54-208/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama3-8b-instruct-finetune-2024-06-08-15-28-54-208\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-434444145045/llama3-8b-instruct-finetune-2024-06-08-15-28-54-208/source/sourcedir.tar.gz\",\"module_name\":\"entry-multi-nodes\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry-multi-nodes.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m entry-multi-nodes\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:03,154 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-06-08 15:36:03,154 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[35mweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed accelerate-0.31.0 aiofiles-23.2.1 aiohttp-3.9.5 aiosignal-1.3.1 altair-5.3.0 anyio-4.4.0 async-timeout-4.0.3 autoawq-0.2.5 autoawq-kernels-0.0.6 bitsandbytes-0.43.1 datasets-2.19.2 deepspeed-0.14.2 dnspython-2.6.1 docstring-parser-0.16 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 frozenlist-1.4.1 fsspec-2024.3.1 gradio-4.36.0 gradio-client-1.0.1 h11-0.14.0 hjson-3.1.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.3 importlib-resources-6.4.0 jieba-0.42.1 llamafactory-0.7.2.dev0 metrics-0.2.6 multidict-6.0.5 orjson-3.10.3 peft-0.11.1 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pydub-0.25.1 pynvml-11.5.0 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 rouge-chinese-1.0.3 ruff-0.4.8 safetensors-0.4.3 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.0 starlette-0.37.2 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 transformers-4.41.2 trl-0.9.4 typer-0.12.3 tyro-0.8.4 ujson-5.10.0 urllib3-2.2.1 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,014 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,014 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,098 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,176 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,253 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,266 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"llama3-8b-instruct-finetune-2024-06-08-15-28-54-208\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-434444145045/llama3-8b-instruct-finetune-2024-06-08-15-28-54-208/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry-multi-nodes\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry-multi-nodes.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=entry-multi-nodes.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=entry-multi-nodes\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-434444145045/llama3-8b-instruct-finetune-2024-06-08-15-28-54-208/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama3-8b-instruct-finetune-2024-06-08-15-28-54-208\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-434444145045/llama3-8b-instruct-finetune-2024-06-08-15-28-54-208/source/sourcedir.tar.gz\",\"module_name\":\"entry-multi-nodes\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry-multi-nodes.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m entry-multi-nodes\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,268 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[35m2024-06-08 15:36:07,268 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[35mObtaining file:///opt/ml/code\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: llamafactory\u001b[0m\n",
      "\u001b[34mBuilding editable for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding editable for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for llamafactory: filename=llamafactory-0.7.2.dev0-0.editable-py3-none-any.whl size=18782 sha256=415b64b359061201ebb61a1373991931db7144ff828d7e8043a5901cf2f5f1ab\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-nms_fv8q/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\u001b[0m\n",
      "\u001b[34mSuccessfully built llamafactory\u001b[0m\n",
      "\u001b[34mInstalling collected packages: llamafactory\u001b[0m\n",
      "\u001b[34mAttempting uninstall: llamafactory\u001b[0m\n",
      "\u001b[34mFound existing installation: llamafactory 0.7.2.dev0\u001b[0m\n",
      "\u001b[34mUninstalling llamafactory-0.7.2.dev0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled llamafactory-0.7.2.dev0\u001b[0m\n",
      "\u001b[34mSuccessfully installed llamafactory-0.7.2.dev0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: peft>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: trl>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gradio>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.36.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.30.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.111.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (0.14.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: autoawq in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: metrics in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.43.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rouge-chinese in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (1.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (0.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.23.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2024.5.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[35mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl>=0.8.6->-r requirements.txt (line 5)) (0.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (23.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (5.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gradio-client==1.0.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.27.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (6.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.0.1->gradio>=4.0.0->-r requirements.txt (line 6)) (11.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (0.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.18.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (0.37.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (5.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->-r requirements.txt (line 15)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.52.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (2.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (11.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: autoawq-kernels in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (2.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.19.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m=0.27.2\u001b[0m\n",
      "\u001b[34mCITATION.cff\u001b[0m\n",
      "\u001b[34mDockerfile\u001b[0m\n",
      "\u001b[34mLICENSE\u001b[0m\n",
      "\u001b[34mMakefile\u001b[0m\n",
      "\u001b[34mREADME.md\u001b[0m\n",
      "\u001b[34mREADME_zh.md\u001b[0m\n",
      "\u001b[34massets\u001b[0m\n",
      "\u001b[34mbuild\u001b[0m\n",
      "\u001b[34mcache\u001b[0m\n",
      "\u001b[34mdata\u001b[0m\n",
      "\u001b[34mdocker-compose.yml\u001b[0m\n",
      "\u001b[34mentry-multi-nodes.py\u001b[0m\n",
      "\u001b[34mentry_single_lora.py\u001b[0m\n",
      "\u001b[34mevaluation\u001b[0m\n",
      "\u001b[34mexamples\u001b[0m\n",
      "\u001b[34mpyproject.toml\u001b[0m\n",
      "\u001b[34mrequirements.txt\u001b[0m\n",
      "\u001b[34ms5cmd\u001b[0m\n",
      "\u001b[34msaves\u001b[0m\n",
      "\u001b[34mscripts\u001b[0m\n",
      "\u001b[34msetup.py\u001b[0m\n",
      "\u001b[34msg_config.yaml\u001b[0m\n",
      "\u001b[34msg_config_ds_lora.yaml\u001b[0m\n",
      "\u001b[34msg_config_lora.yaml\u001b[0m\n",
      "\u001b[34msg_config_lora_ds.yaml\u001b[0m\n",
      "\u001b[34msg_config_lora_merge.yaml\u001b[0m\n",
      "\u001b[34msg_config_multl_node_lora_ds.yaml\u001b[0m\n",
      "\u001b[34msg_config_qlora.yaml\u001b[0m\n",
      "\u001b[34msrc\u001b[0m\n",
      "\u001b[34mtests\u001b[0m\n",
      "\u001b[34mtrain_config\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: llamafactory\u001b[0m\n",
      "\u001b[35mBuilding editable for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding editable for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for llamafactory: filename=llamafactory-0.7.2.dev0-0.editable-py3-none-any.whl size=18782 sha256=a7e0c18d9e7741c67a370c126a56747fb88f783fad72e9a96ceac1c0a07e955f\u001b[0m\n",
      "\u001b[35mStored in directory: /tmp/pip-ephem-wheel-cache-9p8y4u9_/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\u001b[0m\n",
      "\u001b[35mSuccessfully built llamafactory\u001b[0m\n",
      "\u001b[35mInstalling collected packages: llamafactory\u001b[0m\n",
      "\u001b[35mAttempting uninstall: llamafactory\u001b[0m\n",
      "\u001b[35mFound existing installation: llamafactory 0.7.2.dev0\u001b[0m\n",
      "\u001b[35mUninstalling llamafactory-0.7.2.dev0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled llamafactory-0.7.2.dev0\u001b[0m\n",
      "\u001b[35mSuccessfully installed llamafactory-0.7.2.dev0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-434444145045/dataset-for-training/train/identity_2.json /opt/ml/code/data/identity_2.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-434444145045/dataset-for-training/train/ruozhiba.json /opt/ml/code/data/ruozhiba.json\u001b[0m\n",
      "\u001b[34m------envs------\u001b[0m\n",
      "\u001b[34mnum_machines:2\u001b[0m\n",
      "\u001b[34mnum_processes:16\u001b[0m\n",
      "\u001b[34mhost_rank:0\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.41.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.19.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: peft>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: trl>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: gradio>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.36.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.30.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.7.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.111.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (23.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (0.14.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: autoawq in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (0.2.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: metrics in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.2.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.43.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rouge-chinese in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (1.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (0.42.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.23.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2024.5.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.4.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl>=0.8.6->-r requirements.txt (line 5)) (0.8.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (23.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (5.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: gradio-client==1.0.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.27.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (6.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.10.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.0.1->gradio>=4.0.0->-r requirements.txt (line 6)) (11.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (0.14.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.18.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (0.37.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (0.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (5.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (2.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->-r requirements.txt (line 15)) (4.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.52.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (2.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (1.11.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (11.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.22.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: autoawq-kernels in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.18.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.22.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (2.6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (0.16)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.19.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.22.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.35.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m=0.27.2\u001b[0m\n",
      "\u001b[35mCITATION.cff\u001b[0m\n",
      "\u001b[35mDockerfile\u001b[0m\n",
      "\u001b[35mLICENSE\u001b[0m\n",
      "\u001b[35mMakefile\u001b[0m\n",
      "\u001b[35mREADME.md\u001b[0m\n",
      "\u001b[35mREADME_zh.md\u001b[0m\n",
      "\u001b[35massets\u001b[0m\n",
      "\u001b[35mbuild\u001b[0m\n",
      "\u001b[35mcache\u001b[0m\n",
      "\u001b[35mdata\u001b[0m\n",
      "\u001b[35mdocker-compose.yml\u001b[0m\n",
      "\u001b[35mentry-multi-nodes.py\u001b[0m\n",
      "\u001b[35mentry_single_lora.py\u001b[0m\n",
      "\u001b[35mevaluation\u001b[0m\n",
      "\u001b[35mexamples\u001b[0m\n",
      "\u001b[35mpyproject.toml\u001b[0m\n",
      "\u001b[35mrequirements.txt\u001b[0m\n",
      "\u001b[35ms5cmd\u001b[0m\n",
      "\u001b[35msaves\u001b[0m\n",
      "\u001b[35mscripts\u001b[0m\n",
      "\u001b[35msetup.py\u001b[0m\n",
      "\u001b[35msg_config.yaml\u001b[0m\n",
      "\u001b[35msg_config_ds_lora.yaml\u001b[0m\n",
      "\u001b[35msg_config_lora.yaml\u001b[0m\n",
      "\u001b[35msg_config_lora_ds.yaml\u001b[0m\n",
      "\u001b[35msg_config_lora_merge.yaml\u001b[0m\n",
      "\u001b[35msg_config_multl_node_lora_ds.yaml\u001b[0m\n",
      "\u001b[35msg_config_qlora.yaml\u001b[0m\n",
      "\u001b[35msrc\u001b[0m\n",
      "\u001b[35mtests\u001b[0m\n",
      "\u001b[35mtrain_config\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-434444145045/dataset-for-training/train/identity_2.json /opt/ml/code/data/identity_2.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-434444145045/dataset-for-training/train/ruozhiba.json /opt/ml/code/data/ruozhiba.json\u001b[0m\n",
      "\u001b[35m------envs------\u001b[0m\n",
      "\u001b[35mnum_machines:2\u001b[0m\n",
      "\u001b[35mnum_processes:16\u001b[0m\n",
      "\u001b[35mhost_rank:1\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:26,975] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mdf: /root/.triton/autotune\u001b[0m\n",
      "\u001b[34m: No such file or directory\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:29 - INFO - llamafactory.cli - Initializing distributed tasks at: 10.0.212.195:29500\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:30,577] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:30,577] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:30,577] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:30,577] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:30,518] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35mdf: /root/.triton/autotune\u001b[0m\n",
      "\u001b[35m: No such file or directory\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:32 - INFO - llamafactory.cli - Initializing distributed tasks at: 10.0.212.195:29500\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:34,073] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:34,073] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:34,073] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:34,073] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:45,999] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,001] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,005] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,040] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,122] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,141] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,239] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:46,278] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[35m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,265] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,285] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,287] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,305] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,419] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,445] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,584] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-06-08 15:36:48,586] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:48 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:48 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:49,401 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[WARNING|logging.py:314] 2024-06-08 15:36:49,714 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m[WARNING|logging.py:314] 2024-06-08 15:36:49,714 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 91 examples [00:00, 14136.88 examples/s]\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:49 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  95%|█████████▍| 86/91 [00:00<00:00, 825.90 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 492.86 examples/s]\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:50 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 4898 examples [00:00, 95223.91 examples/s]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,091] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,102] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,112] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,112] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,121] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,141] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,141] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:47,144] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,375] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,380] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,390] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,399] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,427] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,427] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,488] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,502] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:36:49,503] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:50 - INFO - llamafactory.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,803 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,803 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,804 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,804 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,803 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,803 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,804 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 15:36:50,804 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 5452.65 examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:314] 2024-06-08 15:36:51,120 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:314] 2024-06-08 15:36:51,120 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 91 examples [00:00, 16232.79 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 884.33 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 506.73 examples/s]\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:51 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4898 examples [00:00, 96528.61 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35malgo-2:376:376 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:380:380 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:377:377 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:375:375 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:379:379 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:374:374 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:381:381 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:376:376 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:380:380 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:377:377 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:379:379 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:378:378 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:375:375 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:381:381 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:374:374 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:378:378 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[35malgo-2:376:376 [2] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:379:379 [5] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:374:374 [0] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:375:375 [1] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:381:381 [7] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:380:380 [6] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:378:378 [4] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:377:377 [3] NCCL INFO Bootstrap : Using eth0:10.0.244.247<0>\u001b[0m\n",
      "\u001b[35malgo-2:374:374 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:374:374 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:380:380 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:379:379 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:377:377 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:380:380 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:379:379 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:376:376 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:377:377 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:375:375 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:376:376 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:381:381 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:375:375 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:381:381 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[35malgo-2:378:378 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:378:378 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:00<00:00, 9333.92 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 5563.39 examples/s]\u001b[0m\n",
      "\u001b[34malgo-1:378:378 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:378:378 [0] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:378:378 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:378:378 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:378:378 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:382:382 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:379:379 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:383:383 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:384:384 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:381:381 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:385:385 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:382:382 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:380:380 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:381:381 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:384:384 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:379:379 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:383:383 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:385:385 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:380:380 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:381:381 [3] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:382:382 [4] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:383:383 [5] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:380:380 [2] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:379:379 [1] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:384:384 [6] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:385:385 [7] NCCL INFO Bootstrap : Using eth0:10.0.212.195<0>\u001b[0m\n",
      "\u001b[34malgo-1:382:382 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:382:382 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:379:379 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:381:381 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:381:381 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:379:379 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:384:384 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:380:380 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:384:384 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:380:380 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:383:383 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:385:385 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:383:383 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:385:385 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO comm 0x56057c23a3c0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO comm 0x565117c95210 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO comm 0x55cadaaad890 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO comm 0x55b5e0a27490 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO comm 0x55c59bf26f00 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO comm 0x55b292c83820 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO comm 0x55ba2355ddf0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO comm 0x55f545497940 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->8\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1368 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Channel 00 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Channel 01 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Channel 00 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1362 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:385:1362 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Channel 01/0 : 7[7] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Channel 01 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 00/0 : 15[7] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:1368 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 01/0 : 15[7] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Channel 00 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Channel 01 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:378:1368 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:1368 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:1368 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:1368 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Channel 00 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Channel 01 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Channel 00 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Channel 01 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:378:1354 [0] NCCL INFO comm 0x55f545497940 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:380:1360 [2] NCCL INFO comm 0x55cadaaad890 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:382:1359 [4] NCCL INFO comm 0x55ba2355ddf0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:384:1356 [6] NCCL INFO comm 0x565117c95210 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:385:1357 [7] NCCL INFO comm 0x56057c23a3c0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:383:1361 [5] NCCL INFO comm 0x55b5e0a27490 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:379:1355 [1] NCCL INFO comm 0x55c59bf26f00 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:381:1358 [3] NCCL INFO comm 0x55b292c83820 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Running on g5.48xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Internode latency set at 0.0 us\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO comm 0x55816eedc5e0 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO comm 0x562a88913fb0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO comm 0x55adaf06e260 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO comm 0x55d91831a570 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO comm 0x557c87592e90 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO comm 0x5640aa1d68b0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO comm 0x557f3f2bc210 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO comm 0x55e69e187bf0 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x97944e32a8255849 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/0/-1->8->-1\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1363 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Channel 00 : 9[1] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Channel 01 : 9[1] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Channel 00 : 14[6] -> 15[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Channel 00 : 13[5] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Channel 00 : 12[4] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Channel 01 : 14[6] -> 15[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Channel 01 : 13[5] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Channel 01 : 12[4] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Channel 00 : 10[2] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Channel 00 : 11[3] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:381:1360 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Channel 00/0 : 15[7] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:381:1360 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Channel 01 : 10[2] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Channel 01/0 : 15[7] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Channel 01 : 11[3] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:1363 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 01/0 : 7[7] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 00 : 8[0] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 01 : 8[0] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Channel 00 : 13[5] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Channel 00 : 12[4] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Channel 01 : 13[5] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Channel 01 : 12[4] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Channel 00 : 11[3] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Channel 01 : 11[3] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Channel 00 : 10[2] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Channel 01 : 10[2] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Channel 00 : 15[7] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Channel 01 : 15[7] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:374:1363 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:1363 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:1363 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:1363 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Channel 00 : 14[6] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Channel 01 : 14[6] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Channel 00 : 9[1] -> 8[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Channel 01 : 9[1] -> 8[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:380:1353 [6] NCCL INFO comm 0x562a88913fb0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:378:1357 [4] NCCL INFO comm 0x55d91831a570 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:376:1354 [2] NCCL INFO comm 0x5640aa1d68b0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:379:1358 [5] NCCL INFO comm 0x55adaf06e260 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:374:1351 [0] NCCL INFO comm 0x55e69e187bf0 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:381:1355 [7] NCCL INFO comm 0x55816eedc5e0 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:377:1352 [3] NCCL INFO comm 0x557c87592e90 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:375:1356 [1] NCCL INFO comm 0x557f3f2bc210 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x97944e32a8255849 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:52 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  66%|██████▌   | 60/91 [00:00<00:00, 588.42 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  78%|███████▊  | 71/91 [00:00<00:00, 703.83 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 475.86 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  59%|█████▉    | 54/91 [00:00<00:00, 525.17 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  66%|██████▌   | 60/91 [00:00<00:00, 588.47 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 472.61 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  59%|█████▉    | 54/91 [00:00<00:00, 537.05 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 474.07 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 461.52 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 433.18 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 431.14 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 431.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 408.79 examples/s]\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 412.14 examples/s]\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 395.25 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 392.22 examples/s]\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[34m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  71%|███████▏  | 65/91 [00:00<00:00, 604.29 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 456.18 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 456.17 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 472.85 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  53%|█████▎    | 48/91 [00:00<00:00, 458.48 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 437.72 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 438.11 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 443.80 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 383.56 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 390.35 examples/s]\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 364.53 examples/s]\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 372.98 examples/s]\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35m06/08/2024 15:36:53 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  57%|█████▋    | 566/1000 [00:00<00:00, 5640.67 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:00<00:00, 7218.52 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:00<00:00, 6515.80 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]#015Converting format of dataset (num_proc=16):  50%|█████     | 503/1000 [00:00<00:00, 4922.93 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4836.82 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:00<00:00, 6256.54 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4784.70 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  57%|█████▋    | 566/1000 [00:00<00:00, 5236.18 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4506.19 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:00<00:00, 6587.52 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4505.46 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:09, 111.48 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4589.17 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4575.42 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4337.08 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:00<00:00, 6093.34 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:00<00:00, 6258.82 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:00<00:00, 6239.29 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:00<00:00, 6162.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 97.11 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4862.23 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:00<00:00, 7354.62 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:00<00:00, 6781.86 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:00<00:00, 7364.82 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4874.94 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4785.91 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4659.00 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4783.38 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4626.91 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4486.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 171.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 230.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 280.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 304.14 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 189.92 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:00<00:03, 244.62 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 282.81 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 308.82 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 328.06 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 341.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 325.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 353.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 374.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 389.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 379.77 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 349.94 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 356.53 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 358.83 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 362.69 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 365.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 395.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 403.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 489.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 468.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 339.73 examples/s]\u001b[0m\n",
      "\u001b[34minput_ids:\u001b[0m\n",
      "\u001b[34m[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 11188, 24406, 11, 459, 15592, 18328, 8040, 555, 11188, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\u001b[0m\n",
      "\u001b[34minputs:\u001b[0m\n",
      "\u001b[34m<|begin_of_text|><|start_header_id|>system<|end_header_id|>\u001b[0m\n",
      "\u001b[34mYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
      "\u001b[34mhi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\u001b[0m\n",
      "\u001b[34mHello! I am RiverBot, an AI assistant developed by River. How can I assist you today?<|eot_id|>\u001b[0m\n",
      "\u001b[34mlabel_ids:\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 11188, 24406, 11, 459, 15592, 18328, 8040, 555, 11188, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\u001b[0m\n",
      "\u001b[34mlabels:\u001b[0m\n",
      "\u001b[34mHello! I am RiverBot, an AI assistant developed by River. How can I assist you today?<|eot_id|>\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 366.55 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  88%|████████▊ | 955/1091 [00:03<00:00, 368.41 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 377.27 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 373.79 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 315.81 examples/s]\u001b[0m\n",
      "\u001b[35minput_ids:\u001b[0m\n",
      "\u001b[35m[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 11188, 24406, 11, 459, 15592, 18328, 8040, 555, 11188, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\u001b[0m\n",
      "\u001b[35minputs:\u001b[0m\n",
      "\u001b[35m<|begin_of_text|><|start_header_id|>system<|end_header_id|>\u001b[0m\n",
      "\u001b[35mYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
      "\u001b[35mhi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\u001b[0m\n",
      "\u001b[35mHello! I am RiverBot, an AI assistant developed by River. How can I assist you today?<|eot_id|>\u001b[0m\n",
      "\u001b[35mlabel_ids:\u001b[0m\n",
      "\u001b[35m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 11188, 24406, 11, 459, 15592, 18328, 8040, 555, 11188, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\u001b[0m\n",
      "\u001b[35mlabels:\u001b[0m\n",
      "\u001b[35mHello! I am RiverBot, an AI assistant developed by River. How can I assist you today?<|eot_id|>\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:733] 2024-06-08 15:36:57,308 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:733] 2024-06-08 15:36:57,308 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:796] 2024-06-08 15:36:57,309 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:796] 2024-06-08 15:36:57,309 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3474] 2024-06-08 15:36:57,399 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3474] 2024-06-08 15:36:57,399 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:08, 124.54 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:08, 118.15 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:09, 102.42 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:09, 102.90 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 100.94 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 98.81 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 98.45 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 182.78 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 164.79 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 178.28 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 178.80 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 161.18 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 170.64 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 179.41 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 247.79 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:04, 217.97 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 225.58 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 230.88 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 242.27 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:04, 213.78 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 222.12 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 295.02 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 275.92 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:03, 271.59 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 274.33 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 330.96 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:03, 243.82 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 315.69 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 285.93 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 301.12 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:01, 356.95 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:04, 200.12 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 277.56 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:01, 344.62 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 321.34 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 299.00 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 322.61 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 375.09 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 348.08 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 314.35 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 275.25 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 389.62 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 329.13 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:02, 304.71 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 356.86 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 323.36 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 330.48 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:01<00:01, 398.56 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:02, 286.87 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 375.43 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:02<00:01, 306.80 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:02<00:01, 333.04 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 404.65 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 333.85 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:02<00:01, 308.95 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 352.38 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 388.35 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 322.71 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 341.21 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 410.65 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 325.63 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 338.66 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 365.27 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 396.63 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 332.47 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 392.90 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 377.34 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 444.02 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 332.85 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 383.23 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:733] 2024-06-08 15:36:57,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:733] 2024-06-08 15:36:57,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:796] 2024-06-08 15:36:57,315 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:796] 2024-06-08 15:36:57,315 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3474] 2024-06-08 15:36:57,400 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3474] 2024-06-08 15:36:57,400 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:08, 118.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:08, 116.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:09, 112.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:08, 117.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 101.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 97.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:10, 100.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:04, 196.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 174.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 182.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:05, 166.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:06, 152.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:06, 152.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:06, 147.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:00<00:03, 247.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 246.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:03, 231.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:04, 220.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 304.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:01<00:04, 207.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 297.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 284.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:02, 293.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:03, 268.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:03, 271.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  25%|██▌       | 275/1091 [00:01<00:03, 259.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 329.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 311.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 270.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 301.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 263.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:02, 268.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:01, 356.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:03, 239.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:01, 341.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 302.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 334.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 295.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:02, 280.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 355.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 363.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 355.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 329.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 358.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 322.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 376.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 351.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:02, 300.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 359.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 363.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 361.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 344.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 388.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 366.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 374.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 365.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:02<00:01, 315.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 360.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 361.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 396.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 380.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 387.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 362.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:02<00:01, 327.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 463.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 356.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 402.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 396.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 389.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 366.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 336.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 356.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 407.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 403.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 397.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 344.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 364.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:00, 340.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 371.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 408.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 406.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 400.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 361.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 364.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 344.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 356.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 498.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 474.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 470.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  88%|████████▊ | 955/1091 [00:03<00:00, 365.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 442.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:03<00:00, 348.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 472.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 482.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 461.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 465.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 340.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 443.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 345.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 368.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 332.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  88%|████████▊ | 955/1091 [00:03<00:00, 350.88 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:01, 339.64 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 393.02 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:02<00:01, 271.24 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 385.83 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 376.27 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:01, 315.71 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 348.35 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 391.52 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:02<00:01, 292.82 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 487.64 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:02<00:00, 350.45 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 487.26 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:02<00:00, 315.80 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:03<00:00, 351.32 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 466.31 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  75%|███████▌  | 819/1091 [00:03<00:00, 309.33 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 477.20 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 425.05 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 350.29 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 434.01 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:03<00:00, 330.53 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  88%|████████▊ | 955/1091 [00:03<00:00, 360.48 examples/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 459.89 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  88%|████████▊ | 955/1091 [00:03<00:00, 415.78 examples/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 334.06 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 417.11 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 335.29 examples/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 321.37 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  88%|████████▊ | 955/1091 [00:03<00:00, 335.25 examples/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 345.23 examples/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 385.12 examples/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 354.80 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 333.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 437.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 328.20 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 322.26 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 345.41 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:03<00:00, 337.91 examples/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 311.16 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 342.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 290.38 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 386.68 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 297.63 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 294.77 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 349.12 examples/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:03<00:00, 284.19 examples/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:12<00:38, 12.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:26,  8.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:25,  8.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:26,  8.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:26,  8.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:26,  8.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:26,  8.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:08<00:26,  8.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:13<00:41, 13.82s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:09<00:29,  9.91s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:10<00:30, 10.15s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:09<00:28,  9.59s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:09<00:29,  9.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:09<00:28,  9.45s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:10<00:30, 10.05s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  25%|██▌       | 1/4 [00:09<00:28,  9.60s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:25<00:25, 12.65s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:21, 10.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:22, 11.07s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:22, 11.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:22, 11.08s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:22, 11.09s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:22, 11.09s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:21<00:21, 10.90s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.55s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:23<00:23, 11.97s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:22<00:23, 11.79s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:23<00:23, 11.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:23<00:24, 12.04s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:22<00:23, 11.81s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:23<00:23, 11.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 2/4 [00:22<00:23, 11.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:37<00:12, 12.34s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.47s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.48s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.47s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:41<00:00,  8.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:41<00:00, 10.27s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3614] 2024-06-08 15:37:38,485 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3614] 2024-06-08 15:37:38,485 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  8.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  9.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  8.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  9.32s/it]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 15:37:38,494 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 15:37:38,494 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:36<00:00,  8.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:36<00:00,  9.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  8.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  9.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  8.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  9.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  8.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  9.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  8.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:37<00:00,  9.27s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.41s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.42s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.56s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.46s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.55s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.57s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.49s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  75%|███████▌  | 3/4 [00:36<00:12, 12.64s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:43<00:00,  9.42s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:43<00:00, 10.93s/it]\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3614] 2024-06-08 15:37:41,116 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3614] 2024-06-08 15:37:41,116 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  8.81s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  8.90s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  9.98s/it]\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:962] 2024-06-08 15:37:41,126 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:962] 2024-06-08 15:37:41,126 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  8.88s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  9.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:40<00:00,  8.91s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:40<00:00, 10.01s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  8.84s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  8.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  9.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  8.84s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 4/4 [00:39<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:37:41,653] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.21s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.02s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.01s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.02s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.02s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.13s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.15s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.17s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.19s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.72s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.76s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.77s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.77s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:14<00:13,  6.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:58 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:58 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:58 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.54s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.54s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.55s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.57s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.57s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.57s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.57s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.51s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4280] 2024-06-08 15:37:59,101 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4280] 2024-06-08 15:37:59,101 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4288] 2024-06-08 15:37:59,101 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\u001b[0m\n",
      "\u001b[35mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4288] 2024-06-08 15:37:59,101 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\u001b[0m\n",
      "\u001b[35mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:917] 2024-06-08 15:37:59,119 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:917] 2024-06-08 15:37:59,119 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:962] 2024-06-08 15:37:59,120 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:962] 2024-06-08 15:37:59,120 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[35m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.36s/it]\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:37:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.33s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4280] 2024-06-08 15:38:03,032 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4280] 2024-06-08 15:38:03,032 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4288] 2024-06-08 15:38:03,032 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4288] 2024-06-08 15:38:03,032 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:917] 2024-06-08 15:38:03,049 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:917] 2024-06-08 15:38:03,049 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 15:38:03,049 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 15:38:03,049 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.adapter - ZeRO3/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:641] 2024-06-08 15:38:03,270 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:641] 2024-06-08 15:38:03,270 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m06/08/2024 15:38:03 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:641] 2024-06-08 15:38:03,289 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:641] 2024-06-08 15:38:03,289 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,516] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,534] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,536] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,536] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,544] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,544] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,544] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,544] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,742] [INFO] [utils.py:779:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,742] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 3.81 GB         CA 2.97 GB         Max_CA 5 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,743] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.01 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,746] [INFO] [stage3.py:130:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,746] [INFO] [stage3.py:131:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,945] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,946] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:03,946] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.02 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 3674112 in 193 params\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,234] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,235] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,235] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.03 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,439] [INFO] [utils.py:779:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,440] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,440] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.03 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,856] [INFO] [utils.py:779:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,857] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:04,857] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.03 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,062] [INFO] [utils.py:779:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,062] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,063] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.03 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,267] [INFO] [utils.py:779:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,268] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,268] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.02 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,473] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,474] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,474] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.03 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,679] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,680] [INFO] [utils.py:780:see_memory_usage] MA 0.94 GB         Max_MA 0.94 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,680] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.02 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,681] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,964] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,965] [INFO] [utils.py:780:see_memory_usage] MA 0.97 GB         Max_MA 0.97 GB         CA 2.97 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,965] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 60.16 GB, percent = 8.0%\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,968] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,968] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,968] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,968] [INFO] [config.py:1000:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f10dc38bf70>\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,969] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   graph_harvesting ............. False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,970] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   world_size ................... 16\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:05,971] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2078] 2024-06-08 15:38:05,971 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2079] 2024-06-08 15:38:05,971 >>   Num examples = 981\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2080] 2024-06-08 15:38:05,971 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2081] 2024-06-08 15:38:05,971 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2084] 2024-06-08 15:38:05,971 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2085] 2024-06-08 15:38:05,971 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2086] 2024-06-08 15:38:05,971 >>   Total optimization steps = 93\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2078] 2024-06-08 15:38:05,971 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2079] 2024-06-08 15:38:05,971 >>   Num examples = 981\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2080] 2024-06-08 15:38:05,971 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2081] 2024-06-08 15:38:05,971 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2084] 2024-06-08 15:38:05,971 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2085] 2024-06-08 15:38:05,971 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2086] 2024-06-08 15:38:05,971 >>   Total optimization steps = 93\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2087] 2024-06-08 15:38:05,974 >>   Number of trainable parameters = 3,407,872\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2087] 2024-06-08 15:38:05,974 >>   Number of trainable parameters = 3,407,872\u001b[0m\n",
      "\u001b[34m0%|          | 0/93 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2078] 2024-06-08 15:38:05,766 >> ***** Running training *****\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2078] 2024-06-08 15:38:05,766 >> ***** Running training *****\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2079] 2024-06-08 15:38:05,766 >>   Num examples = 981\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2079] 2024-06-08 15:38:05,766 >>   Num examples = 981\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2080] 2024-06-08 15:38:05,766 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2081] 2024-06-08 15:38:05,766 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2080] 2024-06-08 15:38:05,766 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2081] 2024-06-08 15:38:05,766 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2084] 2024-06-08 15:38:05,766 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2084] 2024-06-08 15:38:05,766 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2085] 2024-06-08 15:38:05,766 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2086] 2024-06-08 15:38:05,766 >>   Total optimization steps = 93\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2085] 2024-06-08 15:38:05,766 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2086] 2024-06-08 15:38:05,766 >>   Total optimization steps = 93\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2087] 2024-06-08 15:38:05,769 >>   Number of trainable parameters = 3,407,872\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2087] 2024-06-08 15:38:05,769 >>   Number of trainable parameters = 3,407,872\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO bootstrapSplit: rank 7 nranks 16 color 1197013201 key 7 prev 6 next 8 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO comm 0x56059d9594e0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO bootstrapSplit: rank 6 nranks 16 color 1197013201 key 6 prev 5 next 7 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO comm 0x56513a2e3630 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO bootstrapSplit: rank 5 nranks 16 color 1197013201 key 5 prev 4 next 6 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO comm 0x55b602128210 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO bootstrapSplit: rank 4 nranks 16 color 1197013201 key 4 prev 3 next 5 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO comm 0x55ba44099a60 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO bootstrapSplit: rank 3 nranks 16 color 1197013201 key 3 prev 2 next 4 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO comm 0x55b2b52e9170 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO bootstrapSplit: rank 2 nranks 16 color 1197013201 key 2 prev 1 next 3 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO bootstrapSplit: rank 0 nranks 16 color 1197013201 key 0 prev 15 next 1 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO bootstrapSplit: rank 1 nranks 16 color 1197013201 key 1 prev 0 next 2 - DONE\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO comm 0x55cafc1acb90 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO comm 0x55f56ffbf5a0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO comm 0x55c5bd6288d0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->8\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3139 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Channel 00 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Channel 01 : 6[6] -> 7[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Channel 00 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Channel 01 : 5[5] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:385:3140 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:385:3140 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Channel 01/0 : 7[7] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 00/0 : 15[7] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:3139 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 01/0 : 15[7] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Channel 00 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Channel 01 : 5[5] -> 4[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:378:3139 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:3139 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:3139 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:378:3139 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Channel 00 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Channel 01 : 7[7] -> 6[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Channel 00 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Channel 01 : 6[6] -> 5[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:384:3135 [6] NCCL INFO comm 0x56513a2e3630 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:378:3131 [0] NCCL INFO comm 0x55f56ffbf5a0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:380:3132 [2] NCCL INFO comm 0x55cafc1acb90 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:382:3134 [4] NCCL INFO comm 0x55ba44099a60 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:379:3133 [1] NCCL INFO comm 0x55c5bd6288d0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:383:3137 [5] NCCL INFO comm 0x55b602128210 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:381:3138 [3] NCCL INFO comm 0x55b2b52e9170 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:385:3136 [7] NCCL INFO comm 0x56059d9594e0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO bootstrapSplit: rank 15 nranks 16 color 1197013201 key 15 prev 14 next 0 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO bootstrapSplit: rank 14 nranks 16 color 1197013201 key 14 prev 13 next 15 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO comm 0x5581905ded40 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO bootstrapSplit: rank 13 nranks 16 color 1197013201 key 13 prev 12 next 14 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO comm 0x562aaa392e80 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO comm 0x55add256e480 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO bootstrapSplit: rank 8 nranks 16 color 1197013201 key 8 prev 7 next 9 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO comm 0x55e6c0650d40 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO bootstrapSplit: rank 9 nranks 16 color 1197013201 key 9 prev 8 next 10 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO bootstrapSplit: rank 12 nranks 16 color 1197013201 key 12 prev 11 next 13 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO comm 0x557f615aaae0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO comm 0x55d9394e75b0 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO bootstrapSplit: rank 10 nranks 16 color 1197013201 key 10 prev 9 next 11 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO bootstrapSplit: rank 11 nranks 16 color 1197013201 key 11 prev 10 next 12 - DONE\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO comm 0x5640cc844d50 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO comm 0x557ca8c93620 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x3a025a68339918c9 - Init START\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/g5.48xl-topo.xml\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffff0000,00000000,ffffffff,ffff0000,00000000\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,00000000,0000ffff,ffffffff\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/0/-1->8->-1\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3141 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Channel 00 : 14[6] -> 15[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Channel 01 : 14[6] -> 15[7] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Channel 00 : 12[4] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3136 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Channel 00/0 : 15[7] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:381:3136 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Channel 01/0 : 15[7] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Channel 00 : 13[5] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Channel 01 : 12[4] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Channel 01 : 13[5] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Channel 00 : 10[2] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Channel 00 : 9[1] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Channel 00 : 11[3] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Channel 01 : 10[2] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Channel 01 : 9[1] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Channel 01 : 11[3] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:3141 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 01/0 : 7[7] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 00 : 8[0] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 01 : 8[0] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Channel 00 : 13[5] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Channel 01 : 13[5] -> 12[4] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Channel 00 : 12[4] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Channel 01 : 12[4] -> 11[3] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Channel 00 : 11[3] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Channel 01 : 11[3] -> 10[2] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Channel 00 : 10[2] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Channel 01 : 10[2] -> 9[1] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Channel 00 : 15[7] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Channel 01 : 15[7] -> 14[6] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Channel 00 : 14[6] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO P2P is disabled between connected GPUs 6 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Channel 01 : 14[6] -> 13[5] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:374:3141 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:3141 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:3141 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:3141 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Channel 00 : 9[1] -> 8[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Channel 01 : 9[1] -> 8[0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:380:3130 [6] NCCL INFO comm 0x562aaa392e80 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId 1c0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:378:3128 [4] NCCL INFO comm 0x55d9394e75b0 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId 1a0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:376:3133 [2] NCCL INFO comm 0x5640cc844d50 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 180 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:374:3127 [0] NCCL INFO comm 0x55e6c0650d40 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 160 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:379:3131 [5] NCCL INFO comm 0x55add256e480 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId 1b0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:377:3129 [3] NCCL INFO comm 0x557ca8c93620 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 190 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:381:3132 [7] NCCL INFO comm 0x5581905ded40 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId 1d0 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:375:3126 [1] NCCL INFO comm 0x557f615aaae0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 170 commId 0x3a025a68339918c9 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:26,593] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\u001b[0m\n",
      "\u001b[34m1%|          | 1/93 [00:20<31:36, 20.62s/it]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:38:41,859] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/93 [00:35<26:29, 17.47s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/93 [00:51<24:49, 16.55s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/93 [01:06<23:49, 16.06s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/93 [01:22<23:11, 15.81s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 6/93 [01:37<22:41, 15.65s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 7/93 [01:52<22:15, 15.53s/it]\u001b[0m\n",
      "\u001b[34m[2024-06-08 15:40:13,903] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\u001b[0m\n",
      "\u001b[34m9%|▊         | 8/93 [02:07<21:53, 15.45s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 9/93 [02:23<21:34, 15.41s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 10/93 [02:38<21:16, 15.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1818, 'grad_norm': 0.6376478088609574, 'learning_rate': 7e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m11%|█         | 10/93 [02:38<21:16, 15.38s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 11/93 [02:53<21:00, 15.37s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 12/93 [03:09<20:48, 15.41s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 13/93 [03:24<20:30, 15.38s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 14/93 [03:40<20:13, 15.36s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 15/93 [03:55<19:53, 15.30s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 16/93 [04:10<19:41, 15.35s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 17/93 [04:25<19:23, 15.31s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 18/93 [04:41<19:07, 15.30s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 19/93 [04:56<18:53, 15.32s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 20/93 [05:11<18:38, 15.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.076, 'grad_norm': 0.7928726073478104, 'learning_rate': 9.825523265709666e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 20/93 [05:11<18:38, 15.32s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 21/93 [05:27<18:21, 15.30s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 22/93 [05:42<18:05, 15.29s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 23/93 [05:57<17:49, 15.28s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 24/93 [06:12<17:33, 15.27s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 25/93 [06:28<17:18, 15.28s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 26/93 [06:43<17:04, 15.29s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 27/93 [06:58<16:50, 15.30s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 28/93 [07:14<16:33, 15.29s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 29/93 [07:29<16:19, 15.30s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 30/93 [07:44<16:02, 15.28s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8514, 'grad_norm': 0.9858512450529712, 'learning_rate': 9.000127113956674e-05, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 30/93 [07:44<16:02, 15.28s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 31/93 [07:59<15:48, 15.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 32/93 [08:15<15:33, 15.30s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 33/93 [08:30<15:16, 15.28s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 34/93 [08:45<15:03, 15.31s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 35/93 [09:01<14:48, 15.32s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 36/93 [09:16<14:34, 15.35s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 37/93 [09:31<14:18, 15.34s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 38/93 [09:47<14:01, 15.30s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 39/93 [10:02<13:47, 15.33s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 40/93 [10:17<13:31, 15.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7647, 'grad_norm': 1.0302249441727354, 'learning_rate': 7.608457546002424e-05, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 40/93 [10:17<13:31, 15.31s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 41/93 [10:33<13:15, 15.30s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 42/93 [10:48<12:59, 15.28s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 43/93 [11:03<12:44, 15.29s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 44/93 [11:18<12:29, 15.30s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 45/93 [11:34<12:15, 15.32s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 46/93 [11:49<12:00, 15.34s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 47/93 [12:05<11:44, 15.33s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 48/93 [12:20<11:30, 15.33s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 49/93 [12:35<11:15, 15.35s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 50/93 [12:50<10:58, 15.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7319, 'grad_norm': 0.6693632954727355, 'learning_rate': 5.847524671280484e-05, 'epoch': 1.61}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 50/93 [12:50<10:58, 15.31s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 51/93 [13:06<10:41, 15.27s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 52/93 [13:21<10:27, 15.30s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 53/93 [13:36<10:11, 15.28s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 54/93 [13:52<09:57, 15.31s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 55/93 [14:07<09:40, 15.28s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 62/93 [15:54<07:55, 15.34s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 63/93 [16:10<07:39, 15.33s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 64/93 [16:25<07:24, 15.34s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 65/93 [16:40<07:09, 15.32s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 66/93 [16:55<06:53, 15.30s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 67/93 [17:11<06:37, 15.27s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 68/93 [17:26<06:21, 15.26s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 69/93 [17:41<06:06, 15.28s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 70/93 [17:56<05:51, 15.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6848, 'grad_norm': 0.6528935238101884, 'learning_rate': 2.2319914586525777e-05, 'epoch': 2.26}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 70/93 [17:56<05:51, 15.27s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 71/93 [18:12<05:35, 15.27s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 72/93 [18:27<05:20, 15.28s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 73/93 [18:42<05:05, 15.28s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 74/93 [18:58<04:50, 15.30s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 75/93 [19:13<04:35, 15.30s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 76/93 [19:28<04:20, 15.31s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 77/93 [19:44<04:05, 15.32s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 78/93 [19:59<03:49, 15.29s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 79/93 [20:14<03:33, 15.26s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 80/93 [20:29<03:18, 15.29s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6379, 'grad_norm': 0.6340784397079146, 'learning_rate': 8.8921994152595e-06, 'epoch': 2.58}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 80/93 [20:29<03:18, 15.29s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 81/93 [20:45<03:03, 15.28s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 82/93 [21:00<02:48, 15.27s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 83/93 [21:15<02:32, 15.29s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 84/93 [21:30<02:17, 15.28s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 85/93 [21:46<02:01, 15.23s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 86/93 [22:01<01:46, 15.27s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 87/93 [22:16<01:31, 15.30s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 88/93 [22:32<01:16, 15.31s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 89/93 [22:47<01:01, 15.31s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 90/93 [23:02<00:45, 15.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6639, 'grad_norm': 0.8674887167165134, 'learning_rate': 1.2838629803393342e-06, 'epoch': 2.9}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 90/93 [23:02<00:45, 15.33s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 91/93 [23:18<00:30, 15.34s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 92/93 [23:33<00:15, 15.37s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 93/93 [23:48<00:00, 15.37s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2329] 2024-06-08 16:01:54,970 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2329] 2024-06-08 16:01:54,970 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1428.9963, 'train_samples_per_second': 2.059, 'train_steps_per_second': 0.065, 'train_loss': 1.8130380056237663, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 93/93 [23:48<00:00, 15.37s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 93/93 [23:48<00:00, 15.37s/it]\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2329] 2024-06-08 16:01:54,969 >> \u001b[0m\n",
      "\u001b[35mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2329] 2024-06-08 16:01:54,969 >> \u001b[0m\n",
      "\u001b[35mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:3719] 2024-06-08 16:02:12,541 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:3721] 2024-06-08 16:02:12,541 >>   Num examples = 110\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:3724] 2024-06-08 16:02:12,541 >>   Batch size = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:3719] 2024-06-08 16:02:12,541 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:3721] 2024-06-08 16:02:12,541 >>   Num examples = 110\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:3724] 2024-06-08 16:02:12,541 >>   Batch size = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3410] 2024-06-08 16:02:12,534 >> Saving model checkpoint to /tmp/finetuned_model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3410] 2024-06-08 16:02:12,534 >> Saving model checkpoint to /tmp/finetuned_model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:733] 2024-06-08 16:02:12,668 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:733] 2024-06-08 16:02:12,668 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:796] 2024-06-08 16:02:12,669 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:796] 2024-06-08 16:02:12,669 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2513] 2024-06-08 16:02:12,687 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2513] 2024-06-08 16:02:12,687 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2522] 2024-06-08 16:02:12,687 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2522] 2024-06-08 16:02:12,687 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        3.0\n",
      "  total_flos               =    10757GF\n",
      "  train_loss               =      1.813\n",
      "  train_runtime            = 0:23:48.99\n",
      "  train_samples_per_second =      2.059\n",
      "  train_steps_per_second   =      0.065\u001b[0m\n",
      "\u001b[34mFigure saved at: /tmp/finetuned_model/training_loss.png\u001b[0m\n",
      "\u001b[34m06/08/2024 16:02:14 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3719] 2024-06-08 16:02:14,329 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3719] 2024-06-08 16:02:14,329 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3721] 2024-06-08 16:02:14,329 >>   Num examples = 110\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3724] 2024-06-08 16:02:14,329 >>   Batch size = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3721] 2024-06-08 16:02:14,329 >>   Num examples = 110\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3724] 2024-06-08 16:02:14,329 >>   Batch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:04<00:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:08<00:11,  2.90s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:12<00:10,  3.34s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:16<00:07,  3.60s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:20<00:03,  3.78s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:24<00:00,  3.89s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:24<00:00,  3.52s/it]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\u001b[0m\n",
      "\u001b[34mepoch                   =        3.0\n",
      "  eval_loss               =     1.6522\n",
      "  eval_runtime            = 0:00:28.80\n",
      "  eval_samples_per_second =      3.819\u001b[0m\n",
      "\u001b[34meval_steps_per_second   =      0.243\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:450] 2024-06-08 16:02:43,132 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:450] 2024-06-08 16:02:43,132 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34m-----start merge lora-------\u001b[0m\n",
      "\u001b[35m2024-06-08 16:02:52,281 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-06-08 16:02:52,281 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-06-08 16:02:52,281 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[2024-06-08 16:03:03,643] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2108] 2024-06-08 16:03:05,953 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:314] 2024-06-08 16:03:06,282 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:314] 2024-06-08 16:03:06,282 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:06 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:733] 2024-06-08 16:03:06,303 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:733] 2024-06-08 16:03:06,303 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:796] 2024-06-08 16:03:06,304 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:796] 2024-06-08 16:03:06,304 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:06 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3474] 2024-06-08 16:03:06,324 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3474] 2024-06-08 16:03:06,324 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1519] 2024-06-08 16:03:06,325 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1519] 2024-06-08 16:03:06,325 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 16:03:06,326 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 16:03:06,326 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  5.12it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  5.10it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  5.11it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.17it/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4280] 2024-06-08 16:03:07,203 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4280] 2024-06-08 16:03:07,203 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4288] 2024-06-08 16:03:07,203 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4288] 2024-06-08 16:03:07,203 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:917] 2024-06-08 16:03:07,222 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:917] 2024-06-08 16:03:07,222 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 16:03:07,222 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:962] 2024-06-08 16:03:07,222 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:07 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:07 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:07 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:17 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:17 - INFO - llamafactory.model.adapter - Loaded adapter(s): /tmp/finetuned_model\u001b[0m\n",
      "\u001b[34m06/08/2024 16:03:17 - INFO - llamafactory.model.loader - all params: 8030261248\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:472] 2024-06-08 16:03:17,742 >> Configuration saved in /tmp/finetuned_model_merged/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:472] 2024-06-08 16:03:17,742 >> Configuration saved in /tmp/finetuned_model_merged/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:731] 2024-06-08 16:03:17,743 >> Configuration saved in /tmp/finetuned_model_merged/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:731] 2024-06-08 16:03:17,743 >> Configuration saved in /tmp/finetuned_model_merged/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2626] 2024-06-08 16:03:43,264 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model_merged/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2626] 2024-06-08 16:03:43,264 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model_merged/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2513] 2024-06-08 16:03:43,267 >> tokenizer config file saved in /tmp/finetuned_model_merged/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2513] 2024-06-08 16:03:43,267 >> tokenizer config file saved in /tmp/finetuned_model_merged/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2522] 2024-06-08 16:03:43,267 >> Special tokens file saved in /tmp/finetuned_model_merged/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2522] 2024-06-08 16:03:43,267 >> Special tokens file saved in /tmp/finetuned_model_merged/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m-----end merge lora-------\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/config.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/special_tokens_map.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/generation_config.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model.safetensors.index.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/tokenizer_config.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/tokenizer.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00009-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00009-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00008-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00008-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00006-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00006-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00004-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00004-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00003-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00003-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00002-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00002-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00005-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00005-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00007-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00007-of-00009.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model_merged/model-00001-of-00009.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model_merged/model-00001-of-00009.safetensors\u001b[0m\n",
      "\u001b[34m*****************finished training, start cp finetuned model*****************************\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/eval_results.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/eval_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/adapter_config.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/adapter_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/runs/Jun08_15-36-49_algo-1/events.out.tfevents.1717861085.algo-1.378.0 s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/runs/Jun08_15-36-49_algo-1/events.out.tfevents.1717861085.algo-1.378.0\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/trainer_state.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/trainer_log.jsonl s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/trainer_log.jsonl\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/training_loss.png s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/training_loss.png\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/special_tokens_map.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/runs/Jun08_15-36-49_algo-1/events.out.tfevents.1717862563.algo-1.378.1 s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/runs/Jun08_15-36-49_algo-1/events.out.tfevents.1717862563.algo-1.378.1\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/training_args.bin s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/all_results.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/all_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/README.md s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/README.md\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/train_results.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/train_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/tokenizer_config.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/adapter_model.safetensors s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/adapter_model.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/finetuned_model/tokenizer.json s3://sagemaker-us-east-1-434444145045/llama3-8b-lora-sft-ds/output/finetuned_model/tokenizer.json\u001b[0m\n",
      "\u001b[34m-----finished cp-------\u001b[0m\n",
      "\u001b[34m2024-06-08 16:03:54,005 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-08 16:03:54,006 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-08 16:03:54,006 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-08 16:04:16 Uploading - Uploading generated training model\n",
      "2024-06-08 16:04:16 Completed - Training job completed\n",
      "Training seconds: 3914\n",
      "Billable seconds: 3914\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "instance_count = 2\n",
    "instance_type = 'ml.g5.48xlarge' \n",
    "max_time = 3600*24\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# wandb.sagemaker_auth(path=\"./\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'llama3-8b-instruct-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    \"merge_lora\":\"1\", ##是否合并lora模型\n",
    "    \"sg_config\":sg_config,\n",
    "    \"sg_lora_merge_config\":sg_lora_merge_config,\n",
    "    \"s3_data_paths\":f\"{training_input_path}\",\n",
    "    'OUTPUT_MODEL_S3_PATH': destination_s3\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry-multi-nodes.py',\n",
    "                            source_dir='./LLaMA-Factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.2.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            enable_remote_debug=True,\n",
    "                            # keep_alive_period_in_seconds=600,\n",
    "                            max_run=max_time)\n",
    "\n",
    "# # data in channel will be automatically copied to each node - /opt/ml/input/data/train1\n",
    "#input_channel = {'train': f's3://{sagemaker_default_bucket}/datasets/qiandao/{version}/train.json'}\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
