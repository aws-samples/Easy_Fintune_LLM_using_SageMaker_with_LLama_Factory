{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07cc3ce3-e024-4990-b6bb-d22c78514efa",
   "metadata": {},
   "source": [
    "# Using LLama Factory finetune on SageMaker \n",
    "# 2. 使用vLLM进行本地推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ce18b-d8a7-4f31-b8d6-1de1725124af",
   "metadata": {},
   "source": [
    "## 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66561ea-0d2b-440c-ade2-d1820ea01e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q vllm==0.6.1 bitsandbytes transformers==4.45.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a58403-cdd7-495c-8539-7a44757d3d0e",
   "metadata": {},
   "source": [
    "### 从s3下载模型文件到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411aa6cd-fb67-4a1e-a397-2082c87ade0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2eafc8-a6b3-49d0-9dee-349243e3888a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/adapter_config.json to local_model/finetuned_model/adapter_config.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/all_results.json to local_model/finetuned_model/all_results.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/README.md to local_model/finetuned_model/README.md\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/README.md to local_model/finetuned_model/checkpoint-96/README.md\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/scheduler.pt to local_model/finetuned_model/checkpoint-96/scheduler.pt\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/adapter_config.json to local_model/finetuned_model/checkpoint-96/adapter_config.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/rng_state.pth to local_model/finetuned_model/checkpoint-96/rng_state.pth\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/special_tokens_map.json to local_model/finetuned_model/checkpoint-96/special_tokens_map.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/tokenizer_config.json to local_model/finetuned_model/checkpoint-96/tokenizer_config.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/trainer_state.json to local_model/finetuned_model/checkpoint-96/trainer_state.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/training_args.bin to local_model/finetuned_model/checkpoint-96/training_args.bin\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/special_tokens_map.json to local_model/finetuned_model/special_tokens_map.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/tokenizer.json to local_model/finetuned_model/checkpoint-96/tokenizer.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/runs/Nov02_14-19-51_algo-1-ylar8/events.out.tfevents.1730557203.algo-1-ylar8.318.0 to local_model/finetuned_model/runs/Nov02_14-19-51_algo-1-ylar8/events.out.tfevents.1730557203.algo-1-ylar8.318.0\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/tokenizer_config.json to local_model/finetuned_model/tokenizer_config.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/train_results.json to local_model/finetuned_model/train_results.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/adapter_model.safetensors to local_model/finetuned_model/adapter_model.safetensors\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/trainer_log.jsonl to local_model/finetuned_model/trainer_log.jsonl\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/training_args.bin to local_model/finetuned_model/training_args.bin\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/trainer_state.json to local_model/finetuned_model/trainer_state.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/training_loss.png to local_model/finetuned_model/training_loss.png\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/tokenizer.json to local_model/finetuned_model/tokenizer.json\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/adapter_model.safetensors to local_model/finetuned_model/checkpoint-96/adapter_model.safetensors\n",
      "download: s3://sagemaker-us-east-1-901658937252/llama3-8b-qlora/finetuned_model/checkpoint-96/optimizer.pt to local_model/finetuned_model/checkpoint-96/optimizer.pt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://{default_bucket}/llama3-8b-qlora/ ./local_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb1132-2f03-4ee8-82c7-1c5aad5cad24",
   "metadata": {},
   "source": [
    "## 加载模型tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8d2745-b522-4f24-98fc-544c59981a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0dda29-eace-4024-baa9-a31cbe38e6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb16486-c28d-4f1a-8252-b65228d191c3",
   "metadata": {},
   "source": [
    "## 加载sample数据，用于对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce41018c-4d78-421b-a989-8a1cd08001c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "# dataset_name = \"zxbsmk/webnovel_cn\"\n",
    "dataset_name = \"hfl/ruozhiba_gpt4\"\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\",revision='41d2c61beb86c8d4c61916cc656c39d018c40ce5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc42343-da60-47a7-a3c7-5e35bf63c2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4898\n",
      "\n",
      "Training sample:\n",
      "\n",
      "{'input': '', 'instruction': '明天学校要办运动会了，但是我感觉学校根本就是自不量力！ 学校里只有400m的跑道，怎么会有1000米2000米的比赛呢', 'output': '虽然学校只有400米的跑道，但这并不意味着无法进行1000米或2000米的比赛。实际上，这种情况非常常见。通常来说，比赛选手在400米的跑道上完成多圈即可，比如1000米比赛跑2.5圈，2000米比赛则跑5圈。将跑道的距离累积起来达到预定的比赛距离，是学校和其他赛事组织部门常用的方式。这种安排方式也有利于观众集中在一个区域观看比赛而不需要移动位置。'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "num_samples = 200\n",
    "print(train_dataset[randrange(num_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b51667-4bda-49b3-8453-b6602fc650c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_lora_path = './local_model/finetuned_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a54f14-bc6b-4536-af7e-3f402773fb98",
   "metadata": {},
   "source": [
    "## 使用本地的vLLM部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9259ec-03be-46bf-a7bc-b558b5f4d23f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-02 14:45:49 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    }
   ],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "from vllm import LLM,SamplingParams\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d248b96e-6721-44aa-be57-c8ac28633e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "local_model_path = Path(\"./Llama-3-8B-Instruct-AWQ\")\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b590cc15-e64f-4da2-abe2-ff42640cab80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39cf9c61-4c9c-49af-baac-01aaabf695cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 14:46:30 awq_marlin.py:89] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 11-02 14:46:31 config.py:1546] awq_marlin quantization is not tested with LoRA yet.\n",
      "INFO 11-02 14:46:31 llm_engine.py:232] Initializing an LLM engine (v0.6.1) with config: model='TechxGenus/Meta-Llama-3-8B-Instruct-AWQ', speculative_config=None, tokenizer='TechxGenus/Meta-Llama-3-8B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir='Llama-3-8B-Instruct-AWQ/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TechxGenus/Meta-Llama-3-8B-Instruct-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 11-02 14:46:32 model_runner.py:997] Starting to load model TechxGenus/Meta-Llama-3-8B-Instruct-AWQ...\n",
      "INFO 11-02 14:46:32 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6bbd3eb69842358a6f48acefd20fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  16%|#5        | 744M/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554f6fe21ed34f5db6653aecb92acc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  71%|#######   | 744M/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8e3d5855554f5dad4f23ed023c0c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/63.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484c3e57331b4aed9fa467d04c94e35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 14:52:52 model_runner.py:1008] Loading model weights took 5.3492 GB\n",
      "INFO 11-02 14:52:59 gpu_executor.py:122] # GPU blocks: 6466, # CPU blocks: 2048\n",
      "INFO 11-02 14:53:02 model_runner.py:1309] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-02 14:53:02 model_runner.py:1313] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-02 14:53:27 model_runner.py:1428] Graph capturing finished in 25 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id,max_model_len=4096,enable_lora=True,download_dir=str(model_snapshot_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f3ac73-dea3-413c-957a-dc9a615285e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#测试第一个消息\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":\"请始终用中文回答\"},\n",
    "     {\"role\": \"user\", \"content\": \"你是谁？你是干嘛的\"},\n",
    "]\n",
    "\n",
    "#测试第二个消息\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\":\"请始终用中文回答\"},\n",
    "#      {\"role\": \"user\", \"content\": \"睡觉时被女鬼压床我该怎么办？\"},\n",
    "# ]\n",
    "\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236a21c-ce2b-4be6-8ee6-b669dd7a266d",
   "metadata": {},
   "source": [
    "### 使用原始模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d0f474-83bb-4a03-b2bd-b2c4e3c998b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it, est. speed input: 16.57 toks/s, output: 71.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n请始终用中文回答<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n你是谁？你是干嘛的<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Response:\n",
      "'我是 LLaMA，一个由 Meta AI 开发的语言模型。我是一个人工智能语言模型，旨在与用户交谈，回答问题，提供信息和帮助。我的能力包括：\\n\\n* 回答问题：我可以回答各种问题，包括历史、科学、技术、文化、娱乐等领域。\\n* 提供信息：我可以提供相关信息，帮助用户了解某个主题或问题。\\n* 对话：我可以与用户进行对话，回答问题，提供建议和帮助。\\n\\n我是一个机器人，旨在帮助用户获取信息，解决问题和提高语言能力。'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.95,max_tokens=512)\n",
    "\n",
    "outputs = llm.generate(inputs, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt:\\n{prompt!r}\")\n",
    "    print(f\"Response:\\n{generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c25a2-449e-4af5-b1c8-873c92ad9243",
   "metadata": {},
   "source": [
    "### 加载Lora进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a3463f-d618-47ec-a618-89719b210134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_lora_path = './local_model/finetuned_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95db9394-c997-48ce-92c3-33381e837833",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8742/343688066.py:1: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
      "  outputs = llm.generate(inputs, sampling_params,lora_request=LoRARequest(\"adapter\", 1, sql_lora_path))\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 25.11 toks/s, output: 28.45 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n请始终用中文回答<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n你是谁？你是干嘛的<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Response:\n",
      "'您好，我是 Riverbot，一个由 Riverbot 开发的人工智能助手。我可以回答各种问题、提供信息和解决方案来帮助用户。'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, sampling_params,lora_request=LoRARequest(\"adapter\", 1, sql_lora_path))\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt:\\n{prompt!r}\")\n",
    "    print(f\"Response:\\n{generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ce7ba-6ff2-45b7-ba9a-ef909a590d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
